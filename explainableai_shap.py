# -*- coding: utf-8 -*-
"""ExplainableAI_SHAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PWBBW6Gex5iJOmBgdRAZqTvSiO8MjwkN
"""

pip install shap

pip install lime

pip install shapash

pip install eli5

pip install lime>=0.1.1.21

pip install Skope-rules

import pandas as pd

df_lin_reg = pd.read_csv('energydata_complete.csv')

df_lin_reg

del df_lin_reg['date']

df_lin_reg.info()

# y is the dependent variable, that we need to predict

y= df_lin_reg.pop('Appliances')

# X is the set of input features

x = df_lin_reg

import pandas as pd
import shap
import sklearn

# a simple linear model initialized
model = sklearn.linear_model.LinearRegression()

# linear regression model trained
model.fit(x,y)

print("Model coefficients:\n")
for i in range(x.shape[1]):
  print(x.columns[i], "=", model.coef_[i].round(5))

# compute the SHAP values for the linear model
explainer = shap.Explainer(model.predict, x)

# SHAP value calculation
shap_values = explainer(x)

import numpy as np
pd.DataFrame(np.round(shap_values.values,3)).head(3)

# average prediction value is called as the base value
pd.DataFrame(np.round(shap_values.base_values,3)).head(3)

pd.DataFrame(np.round(shap_values.data,3)).head(3)

# getting the partial dependecy plot for SHAP
#Make a standard partial dependence plot for lights on predicted output for row number 20 from the training dataset.
sample_ind = 20
shap.partial_dependence_plot(
    "lights", model.predict, x, model_expected_value=True,
    feature_expected_value=True, ice=False,
    shap_values=shap_values[sample_ind:sample_ind+1,:]
)

# The partial dependency plot is a way to explain the individual predictions and
# generate local interpretations for the sample selected from the dataset
# In this case the sample 20th record is selected from the training dataset

shap.partial_dependence_plot(
    "lights", model.predict, x, ice=False, model_expected_value=True,
    feature_expected_value=True)

# the waterfall_plot shows how we get from shap_values to model.predict(x)
# Local interpretation
[sample_ind]
shap.plots.waterfall(shap_values[sample_ind], max_display=14)

# Computing shap importance values for linear model
feature_names = shap_values.feature_names
shap_df = pd.DataFrame(shap_values.values, columns=feature_names)
vals = np.abs(shap_df.values).mean(0)
shap_importance = pd.DataFrame(list(zip(feature_names, vals)), columns=['col_name','feature_importance_vals'])
shap_importance.sort_values(by=['feature_importance_vals'], ascending=False, inplace=True)
print(shap_importance)

# The feature importance are not scaled hence, the sum of values from all features will not total 100

# The beeswarm chart shows the impact of SHAP on model output
# Blue represents low feature value and red dot shows a high feature value
shap.plots.beeswarm(shap_values)

